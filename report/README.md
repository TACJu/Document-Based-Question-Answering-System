# 互联网数据挖掘大报告
## 生成训练集
### 词向量训练
* 从中文wiki上下载所有条目，使用jieba包进行分词。之后再调用Word2Vec进行词向量训练，词向量大小设置为300维。
* 为了验证利用中文wiki进行训练的优势，我们之后又在作业提供的文本上进行词向量训练，发现前者的MRR会高3%左右。
### 数据预处理
* 对于原始中文文档，使用jieba包进行分词，同时忽略所有的标点符号
* 利用之前训练得到的词向量，构建针对于训练文本的词向量层。具体的做法为训练文本中出现的所有单词分配一个id，同时查找该单词对应的词向量，存入一个新的矩阵。这个矩阵就是之后构建的词向量层，每个单词的id就是在这个单词在词向量层中对应的词向量的索引。
### 训练集生成
* 根据数据预处理中得到的词向量矩阵，为每一个单词分配的id，将原始文档中的所有单词转化成对应的id。
* 为每一个句子设置阶段长度为200，统一大小从而方便之后的训练。
* 利用numpy，分别将经过上述处理的每个句子的id序列存入.npy文件，完成训练集和测试集的生成。
---